{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projappl/project_2013104/pengyan1/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python310.zip',\n",
       " '/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10',\n",
       " '/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages',\n",
       " '/projappl/project_2013104/pengyan1/venv/lib/python3.10/site-packages']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "folder_path = os.path.abspath(\"/projappl/project_2013104/pengyan1/venv/lib/python3.10/site-packages\")\n",
    "if folder_path not in sys.path:\n",
    "    sys.path.append(folder_path)\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MLP, PointNetConv, fps, global_max_pool, radius\n",
    "from torchmetrics.functional import jaccard_index\n",
    "from torchmetrics import JaccardIndex\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import S3DIS\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MLP, knn_interpolate\n",
    "from torch_geometric.typing import WITH_TORCH_CLUSTER\n",
    "from torch_geometric.utils import scatter\n",
    "\n",
    "if not WITH_TORCH_CLUSTER:\n",
    "    quit(\"This example requires 'torch-cluster'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_area = 6  \n",
    "#path = \"/scratch/project_2013104/datasets/s3dis/\" # CSC\n",
    "path = r\"C:\\Users\\yanpe\\OneDrive - Metropolia Ammattikorkeakoulu Oy\\Research\\AI\\scripts\\datasets\\s3dis\" # Local\n",
    "\n",
    "# transform and pre_transform\n",
    "transform = T.Compose([\n",
    "    T.RandomJitter(0.01),\n",
    "    T.RandomRotate(15, axis=0),\n",
    "    T.RandomRotate(15, axis=1),\n",
    "    T.RandomRotate(15, axis=2)\n",
    "])\n",
    "#pre_transform = T.NormalizeScale()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = S3DIS(root=path, test_area=test_area, train=True, force_reload=True, \n",
    "                      transform=transform)#,pre_transform=pre_transform)\n",
    "test_dataset = S3DIS(root=path, test_area=test_area, train=False, force_reload=True, \n",
    "                     transform=transform)#,pre_transform=pre_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_workers=10\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                         num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAModule(torch.nn.Module):\n",
    "    def __init__(self, ratio, r, nn):\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "        self.r = r\n",
    "        self.conv = PointNetConv(nn, add_self_loops=False)\n",
    "\n",
    "    def forward(self, x, pos, batch):\n",
    "        idx = fps(pos, batch, ratio=self.ratio)\n",
    "        row, col = radius(pos, pos[idx], self.r, batch, batch[idx],\n",
    "                          max_num_neighbors=64)\n",
    "        edge_index = torch.stack([col, row], dim=0)\n",
    "        x_dst = None if x is None else x[idx]\n",
    "        x = self.conv((x, x_dst), (pos, pos[idx]), edge_index)\n",
    "        pos, batch = pos[idx], batch[idx]\n",
    "        return x, pos, batch\n",
    "\n",
    "\n",
    "class GlobalSAModule(torch.nn.Module):\n",
    "    def __init__(self, nn):\n",
    "        super().__init__()\n",
    "        self.nn = nn\n",
    "\n",
    "    def forward(self, x, pos, batch):\n",
    "        x = self.nn(torch.cat([x, pos], dim=1))\n",
    "        x = global_max_pool(x, batch)\n",
    "        pos = pos.new_zeros((x.size(0), 3))\n",
    "        batch = torch.arange(x.size(0), device=batch.device)\n",
    "        return x, pos, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPModule(torch.nn.Module):\n",
    "    def __init__(self, k, nn):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.nn = nn\n",
    "\n",
    "    def forward(self, x, pos, batch, x_skip, pos_skip, batch_skip):\n",
    "        x = knn_interpolate(x, pos, pos_skip, batch, batch_skip, k=self.k)\n",
    "        if x_skip is not None:\n",
    "            x = torch.cat([x, x_skip], dim=1)\n",
    "        x = self.nn(x)\n",
    "        return x, pos_skip, batch_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input channels account for both `pos` and node features.\n",
    "        self.sa1_module = SAModule(0.2, 0.2, MLP([3 + 6, 64, 64, 128])) # 3 (pos) + 6 (x)\n",
    "        self.sa2_module = SAModule(0.25, 0.4, MLP([128 + 3, 128, 128, 256]))\n",
    "        self.sa3_module = GlobalSAModule(MLP([256 + 3, 256, 512, 1024]))\n",
    "\n",
    "        self.fp3_module = FPModule(1, MLP([1024 + 256, 256, 256]))\n",
    "        self.fp2_module = FPModule(3, MLP([256 + 128, 256, 128]))\n",
    "        self.fp1_module = FPModule(3, MLP([128 + 6, 128, 128, 128]))\n",
    "\n",
    "        self.mlp = MLP([128, 128, 128, num_classes], dropout=0.5, norm=None)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(128, 128)\n",
    "        self.lin2 = torch.nn.Linear(128, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        sa0_out = (data.x, data.pos, data.batch)\n",
    "        sa1_out = self.sa1_module(*sa0_out)\n",
    "        sa2_out = self.sa2_module(*sa1_out)\n",
    "        sa3_out = self.sa3_module(*sa2_out)\n",
    "\n",
    "        fp3_out = self.fp3_module(*sa3_out, *sa2_out)\n",
    "        fp2_out = self.fp2_module(*fp3_out, *sa1_out)\n",
    "        x, _, _ = self.fp1_module(*fp2_out, *sa0_out)\n",
    "\n",
    "        return self.mlp(x).log_softmax(dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(train_dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = correct_nodes = total_nodes = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        correct_nodes += out.argmax(dim=1).eq(data.y).sum().item()\n",
    "        total_nodes += data.num_nodes\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'[{i+1}/{len(train_loader)}] Loss: {total_loss / 10:.4f} '\n",
    "                  f'Train Acc: {correct_nodes / total_nodes:.4f}')\n",
    "            total_loss = correct_nodes = total_nodes = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    jaccard = JaccardIndex(num_classes=loader.dataset.num_classes, task=\"multiclass\").to(device)\n",
    "    \n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        outs = model(data)\n",
    "        preds = outs.argmax(dim=-1)\n",
    "        jaccard.update(preds, data.y)\n",
    "    \n",
    "    return jaccard.compute().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/318] Loss: 2.2436 Train Acc: 0.3376\n",
      "[20/318] Loss: 1.4151 Train Acc: 0.5946\n",
      "[30/318] Loss: 1.2307 Train Acc: 0.6383\n",
      "[40/318] Loss: 1.1366 Train Acc: 0.6549\n",
      "[50/318] Loss: 1.0166 Train Acc: 0.6951\n",
      "[60/318] Loss: 0.9607 Train Acc: 0.7000\n",
      "[70/318] Loss: 0.9855 Train Acc: 0.6879\n",
      "[80/318] Loss: 0.8485 Train Acc: 0.7296\n",
      "[90/318] Loss: 0.8207 Train Acc: 0.7315\n",
      "[100/318] Loss: 0.7702 Train Acc: 0.7497\n",
      "[110/318] Loss: 0.7468 Train Acc: 0.7555\n",
      "[120/318] Loss: 0.7838 Train Acc: 0.7525\n",
      "[130/318] Loss: 0.7136 Train Acc: 0.7718\n",
      "[140/318] Loss: 0.7100 Train Acc: 0.7703\n",
      "[150/318] Loss: 0.7184 Train Acc: 0.7682\n",
      "[160/318] Loss: 0.6893 Train Acc: 0.7812\n",
      "[170/318] Loss: 0.6894 Train Acc: 0.7811\n",
      "[180/318] Loss: 0.6669 Train Acc: 0.7858\n",
      "[190/318] Loss: 0.7118 Train Acc: 0.7750\n",
      "[200/318] Loss: 0.6702 Train Acc: 0.7887\n",
      "[210/318] Loss: 0.6480 Train Acc: 0.8041\n",
      "[220/318] Loss: 0.6199 Train Acc: 0.8100\n",
      "[230/318] Loss: 0.6074 Train Acc: 0.8136\n",
      "[240/318] Loss: 0.6395 Train Acc: 0.8006\n",
      "[250/318] Loss: 0.5895 Train Acc: 0.8179\n",
      "[260/318] Loss: 0.5959 Train Acc: 0.8126\n",
      "[270/318] Loss: 0.6189 Train Acc: 0.8030\n",
      "[280/318] Loss: 0.5804 Train Acc: 0.8204\n",
      "[290/318] Loss: 0.5559 Train Acc: 0.8293\n",
      "[300/318] Loss: 0.5836 Train Acc: 0.8143\n",
      "[310/318] Loss: 0.5455 Train Acc: 0.8293\n",
      "Epoch: 01, Test IoU: 0.3654, Time: 803.01s\n",
      "[10/318] Loss: 0.5703 Train Acc: 0.8215\n",
      "[20/318] Loss: 0.5605 Train Acc: 0.8253\n",
      "[30/318] Loss: 0.5501 Train Acc: 0.8264\n",
      "[40/318] Loss: 0.5581 Train Acc: 0.8219\n",
      "[50/318] Loss: 0.5427 Train Acc: 0.8355\n",
      "[60/318] Loss: 0.4918 Train Acc: 0.8487\n",
      "[70/318] Loss: 0.5441 Train Acc: 0.8279\n",
      "[80/318] Loss: 0.5661 Train Acc: 0.8248\n",
      "[90/318] Loss: 0.5245 Train Acc: 0.8388\n",
      "[100/318] Loss: 0.5264 Train Acc: 0.8414\n",
      "[110/318] Loss: 0.5040 Train Acc: 0.8447\n",
      "[120/318] Loss: 0.5280 Train Acc: 0.8420\n",
      "[130/318] Loss: 0.5060 Train Acc: 0.8427\n",
      "[140/318] Loss: 0.4861 Train Acc: 0.8503\n",
      "[150/318] Loss: 0.4678 Train Acc: 0.8556\n",
      "[160/318] Loss: 0.5048 Train Acc: 0.8499\n",
      "[170/318] Loss: 0.5023 Train Acc: 0.8452\n",
      "[180/318] Loss: 0.4836 Train Acc: 0.8521\n",
      "[190/318] Loss: 0.4886 Train Acc: 0.8493\n",
      "[200/318] Loss: 0.4698 Train Acc: 0.8585\n",
      "[210/318] Loss: 0.4727 Train Acc: 0.8531\n",
      "[220/318] Loss: 0.4746 Train Acc: 0.8526\n",
      "[230/318] Loss: 0.4890 Train Acc: 0.8470\n",
      "[240/318] Loss: 0.4596 Train Acc: 0.8608\n",
      "[250/318] Loss: 0.4708 Train Acc: 0.8570\n",
      "[260/318] Loss: 0.4892 Train Acc: 0.8479\n",
      "[270/318] Loss: 0.4447 Train Acc: 0.8593\n",
      "[280/318] Loss: 0.4454 Train Acc: 0.8665\n",
      "[290/318] Loss: 0.4552 Train Acc: 0.8567\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m46\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Track epoch start time\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     iou \u001b[38;5;241m=\u001b[39m test(test_loader)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Calculate epoch duration\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 12\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m correct_nodes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39meq(data\u001b[38;5;241m.\u001b[39my)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     14\u001b[0m total_nodes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mnum_nodes\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "for epoch in range(1, 46):\n",
    "    # Track epoch start time\n",
    "    start_time = time.perf_counter()\n",
    "    train()\n",
    "    iou = test(test_loader)\n",
    "    # Calculate epoch duration\n",
    "    epoch_time = time.perf_counter() - start_time\n",
    "    \n",
    "    # Print results with time\n",
    "    print(f'Epoch: {epoch:02d}, Test IoU: {iou:.4f}, Time: {epoch_time:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "checkpoint_path = \"/scratch/project_2013104/checkpoints/pointnet2_s3dis_transform_seg_x6_45_checkpoint.pth\"\n",
    "\n",
    "# Save model, optimizer state, and any other info needed\n",
    "torch.save({\n",
    "    'epoch': 45,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #'loss': loss,\n",
    "    #'test_accuracy': test_acc\n",
    "}, checkpoint_path)\n",
    "\n",
    "print(\"Checkpoint saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
