{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.profiler\n",
    "from pyg_pointnet2 import PyGPointNet2NoColor\n",
    "import loralib as lora\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import time\n",
    "from pc_label_map import color_map\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path of pretrained model\n",
    "pretrained_path = \"checkpoints/pointnet2_s3dis_transform_seg_x3_45_checkpoint.pth\"\n",
    "\n",
    "# File path of LoRA weights\n",
    "lora_path = \"checkpoints/smartlab_lora_weights_x3_50_20250831.pth\"\n",
    "\n",
    "# File path of point cloud to be segmented\n",
    "pcd_path = \"C:/Users/yanpe/OneDrive - Metropolia Ammattikorkeakoulu Oy/Research/data/smartlab/SmartLab_2024_E57_Single_5mm.pcd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(module, r=8, alpha=16, verbose=False):\n",
    "    \"\"\"\n",
    "    Recursively replaces Linear layers with LoRA-enabled layers.\n",
    "    Handles custom modules like MLP, SAModule, and FPModule.\n",
    "    \"\"\"\n",
    "    # Special handling for MLP modules which likely contain multiple linear layers\n",
    "    if hasattr(module, '__class__') and module.__class__.__name__ == 'MLP':\n",
    "        if verbose:\n",
    "            print(f\"Processing MLP module: {module}\")\n",
    "        # Handle linear layers inside the MLP\n",
    "        if hasattr(module, 'lins'):\n",
    "            for i, lin in enumerate(module.lins):\n",
    "                # Check if the layer has the necessary attributes of a Linear layer\n",
    "                if hasattr(lin, 'in_channels') and hasattr(lin, 'out_channels') and hasattr(lin, 'weight'):\n",
    "                    lora_layer = lora.Linear(\n",
    "                        in_features=lin.in_channels,\n",
    "                        out_features=lin.out_channels,\n",
    "                        r=r,\n",
    "                        lora_alpha=alpha\n",
    "                    )\n",
    "                    lora_layer.weight.data = lin.weight.data.clone()\n",
    "                    if hasattr(lin, 'bias') and lin.bias is not None:\n",
    "                        lora_layer.bias.data = lin.bias.data.clone()\n",
    "                    module.lins[i] = lora_layer\n",
    "                    if verbose:\n",
    "                        print(f\"Replaced MLP.lins[{i}] with LoRA ({lin.__class__.__name__})\")\n",
    "\n",
    "    # Process all named children modules\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            # Replace this Linear layer with LoRA\n",
    "            lora_layer = lora.Linear(\n",
    "                in_features=child.in_features,\n",
    "                out_features=child.out_features,\n",
    "                r=r,\n",
    "                lora_alpha=alpha\n",
    "            )\n",
    "            \n",
    "            # Copy original weights and biases\n",
    "            lora_layer.weight.data = child.weight.data.clone()\n",
    "            if child.bias is not None:\n",
    "                lora_layer.bias.data = child.bias.data.clone()\n",
    "            \n",
    "            # Replace the layer\n",
    "            setattr(module, name, lora_layer)\n",
    "            if verbose:\n",
    "                print(f\"Replaced {name} with LoRA\")\n",
    "        elif isinstance(child, nn.Sequential):\n",
    "            # Special handling for Sequential containers\n",
    "            for idx, layer in enumerate(child):\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    lora_layer = lora.Linear(\n",
    "                        in_features=layer.in_features,\n",
    "                        out_features=layer.out_features,\n",
    "                        r=r,\n",
    "                        lora_alpha=alpha\n",
    "                    )\n",
    "                    lora_layer.weight.data = layer.weight.data.clone()\n",
    "                    if layer.bias is not None:\n",
    "                        lora_layer.bias.data = layer.bias.data.clone()\n",
    "                    child[idx] = lora_layer\n",
    "                    if verbose:\n",
    "                        print(f\"Replaced {name}[{idx}] with LoRA\")\n",
    "                else:\n",
    "                    # Recursively apply to layers within Sequential that aren't Linear\n",
    "                    apply_lora(layer, r, alpha, verbose)\n",
    "        else:\n",
    "            # Recursively apply to all other nested submodules\n",
    "            apply_lora(child, r, alpha, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing MLP module: MLP(6, 64, 64, 128)\n",
      "Replaced MLP.lins[0] with LoRA (Linear)\n",
      "Replaced MLP.lins[1] with LoRA (Linear)\n",
      "Replaced MLP.lins[2] with LoRA (Linear)\n",
      "Replaced 0 with LoRA\n",
      "Replaced 1 with LoRA\n",
      "Replaced 2 with LoRA\n",
      "Processing MLP module: MLP(131, 128, 128, 256)\n",
      "Replaced MLP.lins[0] with LoRA (Linear)\n",
      "Replaced MLP.lins[1] with LoRA (Linear)\n",
      "Replaced MLP.lins[2] with LoRA (Linear)\n",
      "Replaced 0 with LoRA\n",
      "Replaced 1 with LoRA\n",
      "Replaced 2 with LoRA\n",
      "Processing MLP module: MLP(259, 256, 512, 1024)\n",
      "Replaced MLP.lins[0] with LoRA (Linear)\n",
      "Replaced MLP.lins[1] with LoRA (Linear)\n",
      "Replaced MLP.lins[2] with LoRA (Linear)\n",
      "Replaced 0 with LoRA\n",
      "Replaced 1 with LoRA\n",
      "Replaced 2 with LoRA\n",
      "Processing MLP module: MLP(1280, 256, 256)\n",
      "Replaced MLP.lins[0] with LoRA (Linear)\n",
      "Replaced MLP.lins[1] with LoRA (Linear)\n",
      "Replaced 0 with LoRA\n",
      "Replaced 1 with LoRA\n",
      "Processing MLP module: MLP(384, 256, 128)\n",
      "Replaced MLP.lins[0] with LoRA (Linear)\n",
      "Replaced MLP.lins[1] with LoRA (Linear)\n",
      "Replaced 0 with LoRA\n",
      "Replaced 1 with LoRA\n",
      "Processing MLP module: MLP(131, 128, 128, 128)\n",
      "Replaced MLP.lins[0] with LoRA (Linear)\n",
      "Replaced MLP.lins[1] with LoRA (Linear)\n",
      "Replaced MLP.lins[2] with LoRA (Linear)\n",
      "Replaced 0 with LoRA\n",
      "Replaced 1 with LoRA\n",
      "Replaced 2 with LoRA\n",
      "Processing MLP module: MLP(128, 128, 128, 13)\n",
      "Replaced MLP.lins[0] with LoRA (Linear)\n",
      "Replaced MLP.lins[1] with LoRA (Linear)\n",
      "Replaced MLP.lins[2] with LoRA (Linear)\n",
      "Replaced 0 with LoRA\n",
      "Replaced 1 with LoRA\n",
      "Replaced 2 with LoRA\n",
      "Replaced lin1 with LoRA\n",
      "Replaced lin2 with LoRA\n",
      "Replaced lin3 with LoRA\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = PyGPointNet2NoColor(num_classes=13).to(device)\n",
    "checkpoint = torch.load(pretrained_path, map_location=device)\n",
    "# Extract the model state dictionary\n",
    "model_state_dict = checkpoint['model_state_dict']\n",
    "model.load_state_dict(model_state_dict, strict=False)\n",
    "# Apply LoRA layers\n",
    "apply_lora(model, r=8, alpha=16, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyGPointNet2NoColor(\n",
       "  (sa1_module): SAModule(\n",
       "    (conv): PointNetConv(local_nn=MLP(6, 64, 64, 128), global_nn=None)\n",
       "  )\n",
       "  (sa2_module): SAModule(\n",
       "    (conv): PointNetConv(local_nn=MLP(131, 128, 128, 256), global_nn=None)\n",
       "  )\n",
       "  (sa3_module): GlobalSAModule(\n",
       "    (nn): MLP(259, 256, 512, 1024)\n",
       "  )\n",
       "  (fp3_module): FPModule(\n",
       "    (nn): MLP(1280, 256, 256)\n",
       "  )\n",
       "  (fp2_module): FPModule(\n",
       "    (nn): MLP(384, 256, 128)\n",
       "  )\n",
       "  (fp1_module): FPModule(\n",
       "    (nn): MLP(131, 128, 128, 128)\n",
       "  )\n",
       "  (mlp): MLP(128, 128, 128, 13)\n",
       "  (lin1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (lin2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (lin3): Linear(in_features=128, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load LoRA weights\n",
    "model.load_state_dict(torch.load(lora_path), strict=False)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters except LoRA\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora_\" in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable LoRA parameters: 44\n",
      "['sa1_module.conv.local_nn.lins.0.lora_A',\n",
      " 'sa1_module.conv.local_nn.lins.0.lora_B',\n",
      " 'sa1_module.conv.local_nn.lins.1.lora_A',\n",
      " 'sa1_module.conv.local_nn.lins.1.lora_B',\n",
      " 'sa1_module.conv.local_nn.lins.2.lora_A',\n",
      " 'sa1_module.conv.local_nn.lins.2.lora_B',\n",
      " 'sa2_module.conv.local_nn.lins.0.lora_A',\n",
      " 'sa2_module.conv.local_nn.lins.0.lora_B',\n",
      " 'sa2_module.conv.local_nn.lins.1.lora_A',\n",
      " 'sa2_module.conv.local_nn.lins.1.lora_B',\n",
      " 'sa2_module.conv.local_nn.lins.2.lora_A',\n",
      " 'sa2_module.conv.local_nn.lins.2.lora_B',\n",
      " 'sa3_module.nn.lins.0.lora_A',\n",
      " 'sa3_module.nn.lins.0.lora_B',\n",
      " 'sa3_module.nn.lins.1.lora_A',\n",
      " 'sa3_module.nn.lins.1.lora_B',\n",
      " 'sa3_module.nn.lins.2.lora_A',\n",
      " 'sa3_module.nn.lins.2.lora_B',\n",
      " 'fp3_module.nn.lins.0.lora_A',\n",
      " 'fp3_module.nn.lins.0.lora_B',\n",
      " 'fp3_module.nn.lins.1.lora_A',\n",
      " 'fp3_module.nn.lins.1.lora_B',\n",
      " 'fp2_module.nn.lins.0.lora_A',\n",
      " 'fp2_module.nn.lins.0.lora_B',\n",
      " 'fp2_module.nn.lins.1.lora_A',\n",
      " 'fp2_module.nn.lins.1.lora_B',\n",
      " 'fp1_module.nn.lins.0.lora_A',\n",
      " 'fp1_module.nn.lins.0.lora_B',\n",
      " 'fp1_module.nn.lins.1.lora_A',\n",
      " 'fp1_module.nn.lins.1.lora_B',\n",
      " 'fp1_module.nn.lins.2.lora_A',\n",
      " 'fp1_module.nn.lins.2.lora_B',\n",
      " 'mlp.lins.0.lora_A',\n",
      " 'mlp.lins.0.lora_B',\n",
      " 'mlp.lins.1.lora_A',\n",
      " 'mlp.lins.1.lora_B',\n",
      " 'mlp.lins.2.lora_A',\n",
      " 'mlp.lins.2.lora_B',\n",
      " 'lin1.lora_A',\n",
      " 'lin1.lora_B',\n",
      " 'lin2.lora_A',\n",
      " 'lin2.lora_B',\n",
      " 'lin3.lora_A',\n",
      " 'lin3.lora_B']\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "trainable_params = [n for n, p in model.named_parameters() if p.requires_grad]\n",
    "from pprint import pprint\n",
    "print(f\"Trainable LoRA parameters: {len(trainable_params)}\")\n",
    "pprint(trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "866900\n"
     ]
    }
   ],
   "source": [
    "# Load point cloud\n",
    "pcd = o3d.io.read_point_cloud(pcd_path)\n",
    "# Move the point cloud to its min(x,y,z) corner \n",
    "def move_to_corner(points):     \n",
    "    min_xyz = points.min(axis=0)    \n",
    "    moved_points = points - min_xyz    \n",
    "    return moved_points\n",
    "\n",
    "moved_points = move_to_corner(np.array(pcd.points))\n",
    "pcd.points = o3d.utility.Vector3dVector(moved_points)\n",
    "\n",
    "# Downsample the point cloud with a voxel of 0.03\n",
    "downpcd = pcd.voxel_down_sample(voxel_size=0.03)\n",
    "# Check size of point cloud\n",
    "print(len(downpcd.points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized coordinates as x features\n",
    "def normalize_points_corner(points):    \n",
    "    min_vals = np.min(points, axis=0)\n",
    "    shifted_points = points - min_vals    \n",
    "    max_vals = np.max(shifted_points, axis=0)\n",
    "    scale = max_vals.copy()    \n",
    "    scale[scale == 0] = 1    \n",
    "    normalized_points = shifted_points / scale\n",
    "    return normalized_points\n",
    "\n",
    "normalized = normalize_points_corner(np.array(downpcd.points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates and colors from the point cloud\n",
    "down_points = torch.tensor(np.array(downpcd.points), dtype=torch.float32)  \n",
    "down_colors = torch.tensor(np.array(downpcd.colors), dtype=torch.float32)\n",
    "down_normalized = torch.tensor(normalized, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Data object with x (3 features) and pos (coordinates)\n",
    "data = Data(x=down_normalized, pos=down_points)\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for inference\n",
    "dataset = [data]  \n",
    "num_workers = 0\n",
    "batch_size = 32\n",
    "# Create a DataLoader (batch_size can be adjusted as needed)\n",
    "custom_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False,\n",
    "                         num_workers=num_workers) #, pin_memory=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts:\n",
      "tensor([[     0, 110516],\n",
      "        [     1, 123230],\n",
      "        [     2,  75714],\n",
      "        [     3,   3887],\n",
      "        [     4, 201839],\n",
      "        [     6,   9118],\n",
      "        [     7,   9390],\n",
      "        [     8,   2160],\n",
      "        [     9,      2],\n",
      "        [    10, 248825],\n",
      "        [    12,  82219]])\n",
      "Total inference time: 242.2589 seconds\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                     torch_cluster::fps         0.00%       1.473ms         0.00%       5.050ms       2.525ms      239.509s        98.87%      239.512s      119.756s             2  \n",
      "                                     torch_cluster::knn         0.00%     860.400us         0.62%        1.503s     501.040ms        1.496s         0.62%        1.503s     501.166ms             3  \n",
      "                                  torch_cluster::radius         0.00%     832.898us         0.30%     730.005ms     365.003ms     720.859ms         0.30%     732.875ms     366.438ms             2  \n",
      "                                  aten::scatter_reduce_         0.00%     506.200us         0.00%     738.100us     246.033us     191.829ms         0.08%     203.442ms      67.814ms             3  \n",
      "                                           aten::linear         0.00%       3.027ms         0.01%      19.394ms     510.363us       2.123ms         0.00%     170.013ms       4.474ms            38  \n",
      "                                            aten::addmm         0.00%       3.521ms         0.00%       3.648ms     192.005us      72.752ms         0.03%      72.884ms       3.836ms            19  \n",
      "                                       aten::batch_norm         0.00%     430.200us         0.00%       3.753ms     375.320us      71.000us         0.00%      45.185ms       4.519ms            10  \n",
      "                           aten::_batch_norm_impl_index         0.00%     645.099us         0.00%       3.323ms     332.300us     113.000us         0.00%      45.114ms       4.511ms            10  \n",
      "                                aten::native_batch_norm         0.00%       1.736ms         0.00%       2.630ms     262.970us      43.536ms         0.02%      44.969ms       4.497ms            10  \n",
      "                                             aten::relu         0.00%     444.400us         0.00%     927.400us      77.283us      97.000us         0.00%      41.504ms       3.459ms            12  \n",
      "                                        aten::clamp_min         0.00%     483.000us         0.00%     483.000us      40.250us      41.407ms         0.02%      41.407ms       3.451ms            12  \n",
      "                                              aten::cat         0.00%       1.466ms         0.00%       2.185ms     115.016us      32.228ms         0.01%      32.591ms       1.715ms            19  \n",
      "                                               aten::to         0.00%       1.516ms         0.10%     235.762ms       3.186ms       1.078ms         0.00%      22.097ms     298.608us            74  \n",
      "                                         aten::_to_copy         0.00%       3.752ms         0.10%     234.246ms       4.183ms       1.732ms         0.00%      21.019ms     375.339us            56  \n",
      "                                            aten::copy_         0.10%     230.466ms         0.10%     230.466ms       3.492ms      18.666ms         0.01%      18.666ms     282.818us            66  \n",
      "                                     aten::scatter_add_         0.00%     849.100us         0.00%     886.600us     110.825us      16.348ms         0.01%      16.440ms       2.055ms             8  \n",
      "                                    aten::masked_select         0.00%       2.866ms         0.92%        2.228s     222.836ms       4.840ms         0.00%      13.537ms       1.354ms            10  \n",
      "                                              aten::mul         0.00%     427.400us         0.00%     427.400us      32.877us      12.465ms         0.01%      12.465ms     958.846us            13  \n",
      "                                         aten::scatter_         0.00%     212.300us         0.00%     220.300us      73.433us      11.568ms         0.00%      11.591ms       3.864ms             3  \n",
      "                                            aten::index         0.00%       2.602ms         0.00%       3.308ms     157.514us       8.956ms         0.00%       9.765ms     465.000us            21  \n",
      "                                     aten::index_select         0.00%     686.298us         0.00%     806.598us     134.433us       9.406ms         0.00%       9.504ms       1.584ms             6  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...         0.00%       2.308ms         0.00%       8.495ms       4.248ms       1.563ms         0.00%       8.493ms       4.247ms             2  \n",
      "                                          aten::nonzero         0.92%        2.223s         0.92%        2.225s     222.454ms       6.174ms         0.00%       7.470ms     747.000us            10  \n",
      "                                              aten::div         0.00%     133.600us         0.00%     133.600us      44.533us       6.192ms         0.00%       6.192ms       2.064ms             3  \n",
      "                                            aten::fill_         0.00%       1.131ms         0.00%       1.131ms      36.484us       5.483ms         0.00%       5.483ms     176.871us            31  \n",
      "                                        aten::new_zeros         0.00%     752.200us         0.00%       2.171ms     155.071us     519.000us         0.00%       4.821ms     344.357us            14  \n",
      "                                            aten::stack         0.00%     644.800us         0.00%       1.124ms     140.462us     281.000us         0.00%       4.294ms     536.750us             8  \n",
      "                                            aten::zero_         0.00%     580.900us         0.00%       1.067ms      46.374us     439.000us         0.00%       4.016ms     174.609us            23  \n",
      "                                             aten::item         0.00%       1.911ms        98.96%      239.732s        6.147s       1.504ms         0.00%       3.763ms      96.487us            39  \n",
      "                                           aten::select         0.00%       2.268ms         0.00%       2.393ms      39.225us       2.381ms         0.00%       3.247ms      53.230us            61  \n",
      "                                              aten::sub         0.00%     325.200us         0.00%     325.200us      46.457us       2.504ms         0.00%       2.504ms     357.714us             7  \n",
      "                                       aten::as_strided         0.00%     490.000us         0.00%     490.000us       2.450us       2.456ms         0.00%       2.456ms      12.280us           200  \n",
      "                                              aten::max         0.00%       1.891ms         0.00%       2.115ms     162.715us       1.610ms         0.00%       2.419ms     186.077us            13  \n",
      "                              aten::_local_scalar_dense        98.96%      239.730s        98.96%      239.730s        6.147s       2.259ms         0.00%       2.259ms      57.923us            39  \n",
      "                                           aten::arange         0.00%       1.308ms         0.00%       2.107ms      95.754us       1.149ms         0.00%       2.243ms     101.955us            22  \n",
      "                                             aten::full         0.00%       1.184ms         0.00%       1.850ms     185.020us     349.000us         0.00%       1.983ms     198.300us            10  \n",
      "                                            aten::empty         0.00%       1.030ms         0.00%       1.030ms       7.359us       1.634ms         0.00%       1.634ms      11.671us           140  \n",
      "                                           aten::unbind         0.00%     689.099us         0.00%       1.506ms     502.033us     358.000us         0.00%       1.610ms     536.667us             3  \n",
      "                                                aten::t         0.00%       1.057ms         0.00%       2.121ms      73.128us     555.000us         0.00%       1.407ms      48.517us            29  \n",
      "                                           aten::narrow         0.00%     689.798us         0.00%       1.463ms      58.512us     577.000us         0.00%       1.385ms      55.400us            25  \n",
      "                                          aten::reshape         0.00%       1.025ms         0.00%       1.140ms      35.634us     741.000us         0.00%       1.171ms      36.594us            32  \n",
      "                                       aten::empty_like         0.00%     392.000us         0.00%     484.300us      40.358us       1.090ms         0.00%       1.167ms      97.250us            12  \n",
      "                                         aten::_unique2         0.00%     658.100us         0.02%      51.826ms      51.826ms     776.000us         0.00%       1.063ms       1.063ms             1  \n",
      "                                            aten::slice         0.00%     879.400us         0.00%     940.700us      32.438us     674.000us         0.00%       1.009ms      34.793us            29  \n",
      "                                    aten::empty_strided         0.00%     515.199us         0.00%     515.199us       7.576us     948.000us         0.00%     948.000us      13.941us            68  \n",
      "                                          aten::resize_         0.00%     482.400us         0.00%     482.400us      12.060us     938.000us         0.00%     938.000us      23.450us            40  \n",
      "                                           aten::cumsum         0.00%     714.799us         0.00%     774.698us      86.078us     734.000us         0.00%     884.000us      98.222us             9  \n",
      "                                        aten::transpose         0.00%     962.799us         0.00%       1.064ms      36.690us     572.000us         0.00%     852.000us      29.379us            29  \n",
      "                                        aten::new_empty         0.00%     512.899us         0.00%     694.199us      36.537us     630.000us         0.00%     838.000us      44.105us            19  \n",
      "                                        aten::expand_as         0.00%     278.500us         0.00%     689.200us      76.578us     103.000us         0.00%     670.000us      74.444us             9  \n",
      "                                           aten::expand         0.00%     386.300us         0.00%     410.700us      45.633us     520.000us         0.00%     567.000us      63.000us             9  \n",
      "                                        aten::ones_like         0.00%      95.900us         0.00%     241.800us     120.900us      77.000us         0.00%     566.000us     283.000us             2  \n",
      "                                             aten::view         0.00%     240.399us         0.00%     240.399us       4.808us     565.000us         0.00%     565.000us      11.300us            50  \n",
      "                                               aten::ne         0.00%     250.200us         0.00%     250.200us      50.040us     484.000us         0.00%     484.000us      96.800us             5  \n",
      "                                            aten::zeros         0.00%      94.300us         0.00%     238.200us     119.100us     106.000us         0.00%     342.000us     171.000us             2  \n",
      "                                          aten::detach_         0.00%     240.000us         0.00%     257.500us      36.786us     203.000us         0.00%     287.000us      41.000us             7  \n",
      "                                              aten::sum         0.00%     241.600us         0.00%     241.600us      80.533us     267.000us         0.00%     267.000us      89.000us             3  \n",
      "                                      aten::log_softmax         0.00%      50.800us         0.00%      84.400us      84.400us      11.000us         0.00%     264.000us     264.000us             1  \n",
      "                                           aten::argmax         0.00%      63.500us         0.00%      65.100us      65.100us     255.000us         0.00%     260.000us     260.000us             1  \n",
      "                                     aten::_log_softmax         0.00%      33.600us         0.00%      33.600us      33.600us     253.000us         0.00%     253.000us     253.000us             1  \n",
      "                                             aten::rand         0.00%     145.900us         0.00%     222.000us     111.000us      72.000us         0.00%     215.000us     107.500us             2  \n",
      "                                            aten::clamp         0.00%     221.800us         0.00%     226.800us      75.600us     175.000us         0.00%     203.000us      67.667us             3  \n",
      "                                     aten::resolve_conj         0.00%       6.500us         0.00%       6.500us       0.591us     189.000us         0.00%     189.000us      17.182us            11  \n",
      "                                       aten::reciprocal         0.00%     215.099us         0.00%     215.099us      71.700us     155.000us         0.00%     155.000us      51.667us             3  \n",
      "                                       aten::lift_fresh         0.00%       5.400us         0.00%       5.400us       0.771us     148.000us         0.00%     148.000us      21.143us             7  \n",
      "                                             aten::set_         0.00%     368.100us         0.00%     368.100us      36.810us     143.000us         0.00%     143.000us      14.300us            10  \n",
      "                                      aten::resolve_neg         0.00%       5.800us         0.00%       5.800us       0.527us     119.000us         0.00%     119.000us      10.818us            11  \n",
      "                                        aten::unsqueeze         0.00%      82.600us         0.00%      89.300us      44.650us      66.000us         0.00%     118.000us      59.000us             2  \n",
      "                                         aten::uniform_         0.00%      64.100us         0.00%      64.100us      32.050us     103.000us         0.00%     103.000us      51.500us             2  \n",
      "                                                detach_         0.00%      17.500us         0.00%      17.500us       2.500us      84.000us         0.00%      84.000us      12.000us             7  \n",
      "                                          aten::dropout         0.00%      24.600us         0.00%      24.600us       1.295us      70.000us         0.00%      70.000us       3.684us            19  \n",
      "                                             aten::ceil         0.00%      64.700us         0.00%      64.700us      32.350us      63.000us         0.00%      63.000us      31.500us             2  \n",
      "                                          aten::random_         0.00%      23.800us         0.00%      23.800us      23.800us       4.000us         0.00%       4.000us       4.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 242.239s\n",
      "Self CUDA time total: 242.245s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Segmentation\n",
    "model.eval()\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "    record_shapes=True,\n",
    ") as prof:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for data in custom_loader:\n",
    "            data = data.to(device)\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                predictions = model(data)\n",
    "            labels = predictions.argmax(dim=-1)\n",
    "            unique_labels, label_counts = torch.unique(labels, return_counts=True)\n",
    "            result_labels = torch.stack((unique_labels, label_counts), dim=1).cpu()\n",
    "            print(\"Label counts:\")\n",
    "            print(result_labels)\n",
    "        end_time = time.time()\n",
    "        print(f\"Total inference time: {end_time - start_time:.4f} seconds\")  \n",
    "\n",
    "# Prediction results    \n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign predicted colors to the point cloud\n",
    "predicted_colors = color_map[labels.cpu().numpy()]  # Shape: [num_points, 3]\n",
    "downpcd.colors = o3d.utility.Vector3dVector(predicted_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the point cloud with colored labels\n",
    "o3d.visualization.draw_geometries([downpcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the point cloud to a file\n",
    "save_path = \"C:/Users/yanpe/OneDrive - Metropolia Ammattikorkeakoulu Oy/Research/data/smartlab/labelled/Smartlab_pcd_lora_label_pointnet2_x3_0.03_20250831.ply\"\n",
    "o3d.io.write_point_cloud(save_path, downpcd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
