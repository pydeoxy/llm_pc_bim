{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the HDF5 File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H5PointCloudDataset(Dataset):\n",
    "    def __init__(self, file_path, transform=None):\n",
    "        # Open the hdf5 file\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            # Load the entire dataset into memory\n",
    "            self.data = np.array(f['data']).astype(np.float32)    # Shape: (num_blocks, 4096, 9)\n",
    "            self.labels = np.array(f['label'])   # Shape: (num_blocks, 4096)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Precompute the number of unique labels (num_classes)\n",
    "        self._num_classes = len(np.unique(self.labels))\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        \"\"\"Return the number of unique labels in the dataset.\"\"\"\n",
    "        return self._num_classes   \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        # Convert the 4096x9 block into a torch tensor\n",
    "        block = torch.from_numpy(self.data[idx])  # Shape: (4096, 9)\n",
    "        label = torch.from_numpy(self.labels[idx]).to(torch.long)  # Shape: (4096)\n",
    "        \n",
    "        # Slice the tensor so that:\n",
    "        #   - pos gets the first 3 columns (x, y, z)\n",
    "        #   - x gets the remaining 6 columns (features)\n",
    "        pos = block[:, :3]\n",
    "        features = block[:, 3:]\n",
    "    \n",
    "        # Replace the first 3 feature values of each point with zeros.\n",
    "        features[:, :3] = 0\n",
    "\n",
    "        data = Data(pos=pos, x=features, y=label)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to the hdf5 dataset\n",
    "script_dir = os.path.dirname(os.getcwd())\n",
    "hdf5_file_path= os.path.join(script_dir, '.', 'docs', 'sim_pc_dataset.h5')\n",
    "\n",
    "# Create the dataset\n",
    "full_dataset = H5PointCloudDataset(hdf5_file_path)\n",
    "\n",
    "# Define split sizes (e.g., 80% training and 20% validation)\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "# Randomly split the dataset\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[4096, 6], y=[4096], pos=[4096, 3])\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(train_dataset.dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True,\n",
    "                          num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=12, shuffle=False,\n",
    "                         num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (x): tensor([[0.0000, 1.0000, 0.0000, 0.1678, 0.0891, 0.0910],\n",
      "        [0.0000, 1.0000, 0.0000, 0.1423, 0.0301, 0.0526],\n",
      "        [0.0000, 0.0000, 1.0000, 0.1228, 0.0266, 0.2259],\n",
      "        ...,\n",
      "        [0.0000, 1.0000, 0.0000, 0.1464, 0.0814, 0.0910],\n",
      "        [0.0000, 0.0000, 1.0000, 0.1374, 0.0087, 0.1010],\n",
      "        [0.0000, 0.0000, 1.0000, 0.1115, 0.0087, 0.2324]])\n",
      "Labels (y): tensor([1, 1, 2,  ..., 1, 2, 2])\n",
      "Number of Unique Labels: tensor([ 1,  2, 12])\n",
      "Positions (pos): tensor([[-3.8881, -5.7588,  0.0000],\n",
      "        [-4.1819, -6.4146, -0.1500],\n",
      "        [-4.4066, -6.4528,  0.5270],\n",
      "        ...,\n",
      "        [-4.1346, -5.8437,  0.0000],\n",
      "        [-4.2386, -6.6528,  0.0390],\n",
      "        [-4.5368, -6.6528,  0.5526]])\n",
      "Center Point: tensor([-4.3428, -6.3116,  0.1144])\n",
      "4096\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming 'dataset' is your PyTorch Geometric dataset\n",
    "index = 4  # Index of the data instance you want to extract\n",
    "\n",
    "# Extract the data instance from the dataset\n",
    "data_instance = train_dataset[index]\n",
    "\n",
    "# Access the features, labels, and positions of this instance\n",
    "x = data_instance.x\n",
    "y = data_instance.y\n",
    "pos = data_instance.pos\n",
    "\n",
    "# Print these values\n",
    "print(\"Features (x):\", x)\n",
    "print(\"Labels (y):\", y)\n",
    "print(\"Number of Unique Labels:\", torch.unique(y))\n",
    "print(\"Positions (pos):\", pos)\n",
    "print(\"Center Point:\", torch.mean(pos, dim=0))\n",
    "\n",
    "print(len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique y values: [ 0  1  2  4  5  6  7  8  9 10 11 12]\n"
     ]
    }
   ],
   "source": [
    "all_labels = []\n",
    "for data in train_dataset:\n",
    "    # If data.y is a tensor, convert it to NumPy\n",
    "    # If data.y is a scalar, you might need to convert it as well.\n",
    "    # Here we assume data.y is a 1D tensor.\n",
    "    all_labels.append(data.y.numpy())\n",
    "\n",
    "# Concatenate all label arrays into one array\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "unique_y = np.unique(all_labels)\n",
    "print(\"Unique y values:\", unique_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training with Simulated Point Cloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MLP, PointNetConv, fps, global_max_pool, radius\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MLP, knn_interpolate\n",
    "from torch_geometric.typing import WITH_TORCH_CLUSTER\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "if not WITH_TORCH_CLUSTER:\n",
    "    quit(\"This example requires 'torch-cluster'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAModule(torch.nn.Module):\n",
    "    def __init__(self, ratio, r, nn):\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "        self.r = r\n",
    "        self.conv = PointNetConv(nn, add_self_loops=False)\n",
    "\n",
    "    def forward(self, x, pos, batch):\n",
    "        idx = fps(pos, batch, ratio=self.ratio)\n",
    "        row, col = radius(pos, pos[idx], self.r, batch, batch[idx],\n",
    "                          max_num_neighbors=64)\n",
    "        edge_index = torch.stack([col, row], dim=0)\n",
    "        x_dst = None if x is None else x[idx]\n",
    "        x = self.conv((x, x_dst), (pos, pos[idx]), edge_index)\n",
    "        pos, batch = pos[idx], batch[idx]\n",
    "        return x, pos, batch\n",
    "\n",
    "\n",
    "class GlobalSAModule(torch.nn.Module):\n",
    "    def __init__(self, nn):\n",
    "        super().__init__()\n",
    "        self.nn = nn\n",
    "\n",
    "    def forward(self, x, pos, batch):\n",
    "        x = self.nn(torch.cat([x, pos], dim=1))\n",
    "        x = global_max_pool(x, batch)\n",
    "        pos = pos.new_zeros((x.size(0), 3))\n",
    "        batch = torch.arange(x.size(0), device=batch.device)\n",
    "        return x, pos, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPModule(torch.nn.Module):\n",
    "    def __init__(self, k, nn):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.nn = nn\n",
    "\n",
    "    def forward(self, x, pos, batch, x_skip, pos_skip, batch_skip):\n",
    "        x = knn_interpolate(x, pos, pos_skip, batch, batch_skip, k=self.k)\n",
    "        if x_skip is not None:\n",
    "            x = torch.cat([x, x_skip], dim=1)\n",
    "        x = self.nn(x)\n",
    "        return x, pos_skip, batch_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input channels account for both `pos` and node features.\n",
    "        self.sa1_module = SAModule(0.2, 0.2, MLP([3 + 6, 64, 64, 128])) # 3 (pos) + 6 (x)\n",
    "        self.sa2_module = SAModule(0.25, 0.4, MLP([128 + 3, 128, 128, 256]))\n",
    "        self.sa3_module = GlobalSAModule(MLP([256 + 3, 256, 512, 1024]))\n",
    "\n",
    "        self.fp3_module = FPModule(1, MLP([1024 + 256, 256, 256]))\n",
    "        self.fp2_module = FPModule(3, MLP([256 + 128, 256, 128]))\n",
    "        self.fp1_module = FPModule(3, MLP([128 + 6, 128, 128, 128]))\n",
    "\n",
    "        self.mlp = MLP([128, 128, 128, num_classes], dropout=0.5, norm=None)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(128, 128)\n",
    "        self.lin2 = torch.nn.Linear(128, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        sa0_out = (data.x, data.pos, data.batch)\n",
    "        sa1_out = self.sa1_module(*sa0_out)\n",
    "        sa2_out = self.sa2_module(*sa1_out)\n",
    "        sa3_out = self.sa3_module(*sa2_out)\n",
    "\n",
    "        fp3_out = self.fp3_module(*sa3_out, *sa2_out)\n",
    "        fp2_out = self.fp2_module(*fp3_out, *sa1_out)\n",
    "        x, _, _ = self.fp1_module(*fp2_out, *sa0_out)\n",
    "\n",
    "        return self.mlp(x).log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(num_classes=13).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = correct_nodes = total_nodes = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        correct_nodes += out.argmax(dim=1).eq(data.y).sum().item()\n",
    "        total_nodes += data.num_nodes\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'[{i+1}/{len(train_loader)}] Loss: {total_loss / 10:.4f} '\n",
    "                  f'Train Acc: {correct_nodes / total_nodes:.4f}')\n",
    "            total_loss = correct_nodes = total_nodes = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import jaccard_index\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    ious = []\n",
    "    \n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        outs = model(data)\n",
    "\n",
    "        sizes = (data.ptr[1:] - data.ptr[:-1]).tolist()\n",
    "        for out, y in zip(outs.split(sizes), data.y.split(sizes)):            \n",
    "            iou = jaccard_index(out.argmax(dim=-1), y, \n",
    "                                num_classes=loader.dataset.num_classes, \n",
    "                                task=\"multiclass\")\n",
    "            ious.append(iou)        \n",
    "\n",
    "    iou = torch.tensor(ious, device=device)\n",
    "\n",
    "    mean_iou = iou.mean()\n",
    "    return float(mean_iou)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 21):\n",
    "    train()\n",
    "    iou = test(test_loader)\n",
    "    print(f'Epoch: {epoch:02d}, Test IoU: {iou:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_checkpoint_path= os.path.join(script_dir, '.', 'docs', 'pointnet2_smartlab_sim_seg_x6_21_checkpoint.pth')\n",
    "\n",
    "# Save model, optimizer state, and any other info needed\n",
    "torch.save({\n",
    "    'epoch': 21,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #'loss': loss,\n",
    "    #'test_accuracy': test_acc\n",
    "}, sim_checkpoint_path)\n",
    "\n",
    "print(\"Checkpoint saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LoRa Fine-tuning with Pre-trained Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(num_classes=13).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoint dictionary\n",
    "checkpoint = torch.load(r\"C:\\Users\\yanpe\\OneDrive - Metropolia Ammattikorkeakoulu Oy\\Research\\AI\\scripts\\checkpoints\\pointnet2_s3dis_transform_seg_x6_27_checkpoint.pth\", map_location=device)\n",
    "# Extract the model state dictionary\n",
    "model_state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# print(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (sa1_module): SAModule(\n",
       "    (conv): PointNetConv(local_nn=MLP(9, 64, 64, 128), global_nn=None)\n",
       "  )\n",
       "  (sa2_module): SAModule(\n",
       "    (conv): PointNetConv(local_nn=MLP(131, 128, 128, 256), global_nn=None)\n",
       "  )\n",
       "  (sa3_module): GlobalSAModule(\n",
       "    (nn): MLP(259, 256, 512, 1024)\n",
       "  )\n",
       "  (fp3_module): FPModule(\n",
       "    (nn): MLP(1280, 256, 256)\n",
       "  )\n",
       "  (fp2_module): FPModule(\n",
       "    (nn): MLP(384, 256, 128)\n",
       "  )\n",
       "  (fp1_module): FPModule(\n",
       "    (nn): MLP(134, 128, 128, 128)\n",
       "  )\n",
       "  (mlp): MLP(128, 128, 128, 13)\n",
       "  (lin1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (lin2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (lin3): Linear(in_features=128, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(model_state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=128, out_features=13, bias=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lin3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x0000025640FAEC00>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import loralib as lora\n",
    "\n",
    "# Load the original model\n",
    "#model = torch.load(\"original_model.pth\")\n",
    "\n",
    "# Freeze the entire base model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Apply LoRA to segmentation head\n",
    "# Replace specific layers with LoRA-enabled layers\n",
    "# Example: Replace a linear layer in the segmentation head\n",
    "model.lin3 = lora.Linear(\n",
    "    in_features=128,        # Original input dimension\n",
    "    out_features=13,       # Original output dimension\n",
    "    r=8,                   # Rank of the low-rank matrices (hyperparameter)\n",
    "    lora_alpha=16,         # Scaling factor for LoRA weights\n",
    "    merge_weights=False     # Keep LoRA separate during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = correct_nodes = total_nodes = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        correct_nodes += out.argmax(dim=1).eq(data.y).sum().item()\n",
    "        total_nodes += data.num_nodes\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'[{i+1}/{len(train_loader)}] Loss: {total_loss / 10:.4f} '\n",
    "                  f'Train Acc: {correct_nodes / total_nodes:.4f}')\n",
    "            total_loss = correct_nodes = total_nodes = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import jaccard_index\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    ious = []\n",
    "    \n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        outs = model(data)\n",
    "\n",
    "        sizes = (data.ptr[1:] - data.ptr[:-1]).tolist()\n",
    "        for out, y in zip(outs.split(sizes), data.y.split(sizes)):            \n",
    "            iou = jaccard_index(out.argmax(dim=-1), y, \n",
    "                                num_classes=loader.dataset.num_classes, \n",
    "                                task=\"multiclass\")\n",
    "            ious.append(iou)        \n",
    "\n",
    "    iou = torch.tensor(ious, device=device)\n",
    "\n",
    "    mean_iou = iou.mean()\n",
    "    return float(mean_iou)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "    train()\n",
    "    iou = test(test_loader)\n",
    "    print(f'Epoch: {epoch:02d}, Test IoU: {iou:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lora.lora_state_dict(model), \"pointcloud_lora_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original model and LoRA weights\n",
    "model = torch.load(\"original_model.pth\")\n",
    "lora_weights = torch.load(\"pointcloud_lora_weights.pth\")\n",
    "\n",
    "# Merge LoRA weights into the base model\n",
    "model.load_state_dict(lora_weights, strict=False)\n",
    "lora.mark_only_lora_as_trainable(model)  # Optional: Revert if needed later\n",
    "\n",
    "# Now use `model` as a standard fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pointnet2 import PointNet2Seg\n",
    "import loralib as lora\n",
    "\n",
    "class PointNet2LoRA(PointNet2Seg):\n",
    "    def __init__(self, num_classes, r=8, alpha=16):\n",
    "        super().__init__(num_classes)\n",
    "        \n",
    "        # Freeze base model\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Replace final MLP layers with LoRA\n",
    "        self.seg_layers = nn.Sequential(\n",
    "            lora.Linear(256, 128, r=r, lora_alpha=alpha),\n",
    "            nn.ReLU(),\n",
    "            lora.Linear(128, num_classes, r=r, lora_alpha=alpha)\n",
    "        )\n",
    "\n",
    "# Usage\n",
    "model = PointNet2LoRA(num_classes=10, r=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
