{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, random_split,Subset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import loralib as lora\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import JaccardIndex\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.typing import WITH_TORCH_CLUSTER\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from pyg_pointnet2 import PyGPointNet2NoColorLoRa\n",
    "from pc_dataset import H5PCDataset\n",
    "\n",
    "\n",
    "if not WITH_TORCH_CLUSTER:\n",
    "    quit(\"This example requires 'torch-cluster'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take out colors\n",
    "class SelectLast3Features:\n",
    "    def __call__(self, data):\n",
    "        # If data.x is defined, select only its last 3 features.\n",
    "        if data.x is not None:\n",
    "            data.x = data.x[:, -3:]\n",
    "        return data\n",
    "\n",
    "# transform and pre_transform\n",
    "transform = T.Compose([\n",
    "    T.RandomJitter(0.01),\n",
    "    T.RandomRotate(15, axis=0),\n",
    "    T.RandomRotate(15, axis=1),\n",
    "    T.RandomRotate(15, axis=2)\n",
    "])\n",
    "\n",
    "pre_transform =  T.Compose([\n",
    "    #T.NormalizeScale(),\n",
    "    SelectLast3Features()\n",
    "    ])\n",
    "\n",
    "full_dataset = H5PCDataset(file_path='../docs/sim_pc_dataset_moved.h5', pre_transform=pre_transform)\n",
    "\n",
    "# Define split sizes (e.g., 80% training and 20% validation)\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "# Randomly split the dataset\n",
    "train_subset, test_subset = random_split(full_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrap train_subset in AugmentedSubset\n",
    "class AugmentedSubset(Subset):\n",
    "    def __init__(self, subset, transform):\n",
    "        super().__init__(subset.dataset, subset.indices)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = super().__getitem__(idx)\n",
    "        return self.transform(data)\n",
    "\n",
    "train_dataset = AugmentedSubset(train_subset, transform)\n",
    "test_dataset = test_subset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[4096, 3], y=[4096], pos=[4096, 3])\n",
      "13\n",
      "Data(x=[4096, 3], y=[4096], pos=[4096, 3])\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(train_dataset.dataset.num_classes)\n",
    "print(test_dataset[0])\n",
    "print(test_dataset.dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_workers=0\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                         num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = PyGPointNet2NoColorLoRa(num_classes=13).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = PyGPointNet2NoColorLoRa(num_classes=13).to(device)\n",
    "model.load_state_dict(torch.load(\"checkpoints/pointnet2_s3dis_transform_seg_x3_45_checkpoint.pth\", map_location=device), strict=False)  # Load pretrained weights\n",
    "\n",
    "# Freeze all parameters except LoRA\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze LoRA parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora_\" in name:  # LoRA parameters have \"lora_A\" or \"lora_B\" in their names\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyGPointNet2NoColorLoRa(\n",
       "  (sa1_module): SAModule(\n",
       "    (conv): PointNetConv(local_nn=Sequential(\n",
       "      (0): Linear(in_features=6, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "    ), global_nn=None)\n",
       "  )\n",
       "  (sa2_module): SAModule(\n",
       "    (conv): PointNetConv(local_nn=Sequential(\n",
       "      (0): Linear(in_features=131, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=128, out_features=256, bias=True)\n",
       "    ), global_nn=None)\n",
       "  )\n",
       "  (sa3_module): GlobalSAModule(\n",
       "    (nn): Sequential(\n",
       "      (0): Linear(in_features=259, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fp3_module): FPModule(\n",
       "    (nn): Sequential(\n",
       "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fp2_module): FPModule(\n",
       "    (nn): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fp1_module): FPModule(\n",
       "    (nn): Sequential(\n",
       "      (0): Linear(in_features=131, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=13, bias=True)\n",
       "  )\n",
       "  (lin1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (lin2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (lin3): Linear(in_features=128, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['sa1_module.conv.local_nn.0.lora_A', 'sa1_module.conv.local_nn.0.lora_B', 'sa1_module.conv.local_nn.2.lora_A', 'sa1_module.conv.local_nn.2.lora_B', 'sa1_module.conv.local_nn.4.lora_A', 'sa1_module.conv.local_nn.4.lora_B', 'sa2_module.conv.local_nn.0.lora_A', 'sa2_module.conv.local_nn.0.lora_B', 'sa2_module.conv.local_nn.2.lora_A', 'sa2_module.conv.local_nn.2.lora_B', 'sa2_module.conv.local_nn.4.lora_A', 'sa2_module.conv.local_nn.4.lora_B', 'sa3_module.nn.0.lora_A', 'sa3_module.nn.0.lora_B', 'sa3_module.nn.2.lora_A', 'sa3_module.nn.2.lora_B', 'sa3_module.nn.4.lora_A', 'sa3_module.nn.4.lora_B', 'fp3_module.nn.0.lora_A', 'fp3_module.nn.0.lora_B', 'fp3_module.nn.2.lora_A', 'fp3_module.nn.2.lora_B', 'fp2_module.nn.0.lora_A', 'fp2_module.nn.0.lora_B', 'fp2_module.nn.2.lora_A', 'fp2_module.nn.2.lora_B', 'fp1_module.nn.0.lora_A', 'fp1_module.nn.0.lora_B', 'fp1_module.nn.2.lora_A', 'fp1_module.nn.2.lora_B', 'fp1_module.nn.4.lora_A', 'fp1_module.nn.4.lora_B', 'mlp.0.lora_A', 'mlp.0.lora_B', 'mlp.3.lora_A', 'mlp.3.lora_B', 'mlp.6.lora_A', 'mlp.6.lora_B', 'lin1.lora_A', 'lin1.lora_B', 'lin2.lora_A', 'lin2.lora_B', 'lin3.lora_A', 'lin3.lora_B']\n"
     ]
    }
   ],
   "source": [
    "# Verify trainable parameters\n",
    "trainable_params = [name for name, p in model.named_parameters() if p.requires_grad]\n",
    "print(\"Trainable parameters:\", trainable_params)\n",
    "\n",
    "# After freezing the base model and enabling LoRA:\n",
    "optimizer = torch.optim.Adam(\n",
    "    [p for p in model.parameters() if p.requires_grad],  # Manual LoRA params\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = correct_nodes = total_nodes = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out.view(-1, 13), data.y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        correct_nodes += out.argmax(dim=1).eq(data.y).sum().item()\n",
    "        total_nodes += data.num_nodes\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'[{i+1}/{len(train_loader)}] Loss: {total_loss / 10:.4f} '\n",
    "                  f'Train Acc: {correct_nodes / total_nodes:.4f}')\n",
    "            total_loss = correct_nodes = total_nodes = 0\n",
    "    # If there are remaining batches that were not printed (i.e., i+1 not divisible by 10)\n",
    "    if total_nodes > 0:\n",
    "        num_remaining = (i + 1) % 10  # Number of batches in the leftover segment\n",
    "        print(f'[{i+1}/{len(train_loader)}] Loss: {total_loss / num_remaining:.4f} '\n",
    "              f'Train Acc: {correct_nodes / total_nodes:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    jaccard = JaccardIndex(num_classes=loader.dataset.dataset.num_classes, task=\"multiclass\").to(device)\n",
    "    \n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        outs = model(data)\n",
    "        preds = outs.argmax(dim=-1)\n",
    "        jaccard.update(preds, data.y)\n",
    "    \n",
    "    return jaccard.compute().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/7] Loss: 1.6813 Train Acc: 0.2542\n",
      "Epoch: 01, Test IoU: 0.0193\n",
      "[7/7] Loss: 1.6009 Train Acc: 0.2761\n",
      "Epoch: 02, Test IoU: 0.0193\n",
      "[7/7] Loss: 1.5211 Train Acc: 0.3065\n",
      "Epoch: 03, Test IoU: 0.0193\n",
      "[7/7] Loss: 1.5526 Train Acc: 0.2851\n",
      "Epoch: 04, Test IoU: 0.0193\n",
      "[7/7] Loss: 1.5755 Train Acc: 0.2722\n",
      "Epoch: 05, Test IoU: 0.0193\n",
      "[7/7] Loss: 1.6244 Train Acc: 0.2961\n",
      "Epoch: 06, Test IoU: 0.0926\n",
      "[7/7] Loss: 1.5062 Train Acc: 0.3371\n",
      "Epoch: 07, Test IoU: 0.0590\n",
      "[7/7] Loss: 1.6104 Train Acc: 0.2763\n",
      "Epoch: 08, Test IoU: 0.0193\n",
      "[7/7] Loss: 1.5407 Train Acc: 0.2585\n",
      "Epoch: 09, Test IoU: 0.0193\n",
      "[7/7] Loss: 1.6135 Train Acc: 0.2765\n",
      "Epoch: 10, Test IoU: 0.0193\n",
      "[7/7] Loss: 1.5731 Train Acc: 0.2980\n",
      "Epoch: 11, Test IoU: 0.0193\n",
      "[7/7] Loss: 1.5640 Train Acc: 0.3167\n",
      "Epoch: 12, Test IoU: 0.0193\n",
      "[7/7] Loss: 1.4819 Train Acc: 0.3152\n",
      "Epoch: 13, Test IoU: 0.0193\n",
      "[7/7] Loss: 1.5210 Train Acc: 0.2625\n",
      "Epoch: 14, Test IoU: 0.0193\n",
      "[7/7] Loss: 1.5317 Train Acc: 0.2621\n",
      "Epoch: 15, Test IoU: 0.0193\n",
      "[7/7] Loss: 1.5195 Train Acc: 0.3255\n",
      "Epoch: 16, Test IoU: 0.0850\n",
      "[7/7] Loss: 1.6381 Train Acc: 0.3030\n",
      "Epoch: 17, Test IoU: 0.0193\n",
      "[7/7] Loss: 1.5831 Train Acc: 0.2679\n",
      "Epoch: 18, Test IoU: 0.0193\n",
      "[7/7] Loss: 1.5049 Train Acc: 0.2736\n",
      "Epoch: 19, Test IoU: 0.0193\n",
      "[7/7] Loss: 1.4712 Train Acc: 0.3238\n",
      "Epoch: 20, Test IoU: 0.0193\n",
      "[7/7] Loss: 1.4965 Train Acc: 0.2940\n",
      "Epoch: 21, Test IoU: 0.0193\n",
      "[7/7] Loss: 1.5838 Train Acc: 0.2874\n",
      "Epoch: 22, Test IoU: 0.0506\n",
      "[7/7] Loss: 1.4535 Train Acc: 0.3747\n",
      "Epoch: 23, Test IoU: 0.0883\n",
      "[7/7] Loss: 1.4308 Train Acc: 0.3633\n",
      "Epoch: 24, Test IoU: 0.0741\n",
      "[7/7] Loss: 1.4791 Train Acc: 0.3712\n",
      "Epoch: 25, Test IoU: 0.0742\n",
      "[7/7] Loss: 1.4838 Train Acc: 0.3896\n",
      "Epoch: 26, Test IoU: 0.0927\n",
      "[7/7] Loss: 1.5990 Train Acc: 0.3996\n",
      "Epoch: 27, Test IoU: 0.0753\n",
      "[7/7] Loss: 1.5097 Train Acc: 0.4154\n",
      "Epoch: 28, Test IoU: 0.0932\n",
      "[7/7] Loss: 1.4473 Train Acc: 0.4640\n",
      "Epoch: 29, Test IoU: 0.0961\n",
      "[7/7] Loss: 1.5565 Train Acc: 0.4421\n",
      "Epoch: 30, Test IoU: 0.0959\n",
      "[7/7] Loss: 1.4063 Train Acc: 0.4801\n",
      "Epoch: 31, Test IoU: 0.0936\n",
      "[7/7] Loss: 1.5143 Train Acc: 0.4875\n",
      "Epoch: 32, Test IoU: 0.1046\n",
      "[7/7] Loss: 1.4917 Train Acc: 0.5176\n",
      "Epoch: 33, Test IoU: 0.0972\n",
      "[7/7] Loss: 1.4966 Train Acc: 0.5151\n",
      "Epoch: 34, Test IoU: 0.1014\n",
      "[7/7] Loss: 1.4339 Train Acc: 0.5190\n",
      "Epoch: 35, Test IoU: 0.0970\n",
      "[7/7] Loss: 1.4694 Train Acc: 0.5264\n",
      "Epoch: 36, Test IoU: 0.0994\n",
      "[7/7] Loss: 1.3490 Train Acc: 0.5246\n",
      "Epoch: 37, Test IoU: 0.0961\n",
      "[7/7] Loss: 1.3159 Train Acc: 0.5161\n",
      "Epoch: 38, Test IoU: 0.0988\n",
      "[7/7] Loss: 1.3107 Train Acc: 0.5245\n",
      "Epoch: 39, Test IoU: 0.0994\n",
      "[7/7] Loss: 1.3021 Train Acc: 0.5319\n",
      "Epoch: 40, Test IoU: 0.0999\n",
      "[7/7] Loss: 1.3063 Train Acc: 0.5308\n",
      "Epoch: 41, Test IoU: 0.0998\n",
      "[7/7] Loss: 1.3117 Train Acc: 0.5326\n",
      "Epoch: 42, Test IoU: 0.1014\n",
      "[7/7] Loss: 1.2862 Train Acc: 0.5347\n",
      "Epoch: 43, Test IoU: 0.1053\n",
      "[7/7] Loss: 1.2378 Train Acc: 0.5460\n",
      "Epoch: 44, Test IoU: 0.1009\n",
      "[7/7] Loss: 1.2435 Train Acc: 0.5622\n",
      "Epoch: 45, Test IoU: 0.1569\n",
      "[7/7] Loss: 1.1954 Train Acc: 0.5814\n",
      "Epoch: 46, Test IoU: 0.1604\n",
      "[7/7] Loss: 1.2331 Train Acc: 0.5981\n",
      "Epoch: 47, Test IoU: 0.1677\n",
      "[7/7] Loss: 1.1578 Train Acc: 0.6127\n",
      "Epoch: 48, Test IoU: 0.1650\n",
      "[7/7] Loss: 1.1179 Train Acc: 0.6386\n",
      "Epoch: 49, Test IoU: 0.1648\n",
      "[7/7] Loss: 1.1697 Train Acc: 0.6539\n",
      "Epoch: 50, Test IoU: 0.1633\n",
      "[7/7] Loss: 1.0688 Train Acc: 0.6518\n",
      "Epoch: 51, Test IoU: 0.1698\n",
      "[7/7] Loss: 1.0840 Train Acc: 0.6688\n",
      "Epoch: 52, Test IoU: 0.1848\n",
      "[7/7] Loss: 1.0891 Train Acc: 0.6747\n",
      "Epoch: 53, Test IoU: 0.1720\n",
      "[7/7] Loss: 1.0130 Train Acc: 0.6826\n",
      "Epoch: 54, Test IoU: 0.1809\n",
      "[7/7] Loss: 1.0933 Train Acc: 0.6545\n",
      "Epoch: 55, Test IoU: 0.1822\n",
      "[7/7] Loss: 1.0991 Train Acc: 0.6784\n",
      "Epoch: 56, Test IoU: 0.1844\n",
      "[7/7] Loss: 1.0687 Train Acc: 0.6710\n",
      "Epoch: 57, Test IoU: 0.1640\n",
      "[7/7] Loss: 1.0833 Train Acc: 0.6718\n",
      "Epoch: 58, Test IoU: 0.1958\n",
      "[7/7] Loss: 1.0784 Train Acc: 0.6896\n",
      "Epoch: 59, Test IoU: 0.1846\n",
      "[7/7] Loss: 1.0177 Train Acc: 0.6751\n",
      "Epoch: 60, Test IoU: 0.1915\n",
      "[7/7] Loss: 0.9739 Train Acc: 0.7046\n",
      "Epoch: 61, Test IoU: 0.1892\n",
      "[7/7] Loss: 1.0230 Train Acc: 0.7092\n",
      "Epoch: 62, Test IoU: 0.1867\n",
      "[7/7] Loss: 0.9825 Train Acc: 0.6903\n",
      "Epoch: 63, Test IoU: 0.1741\n",
      "[7/7] Loss: 0.9802 Train Acc: 0.6829\n",
      "Epoch: 64, Test IoU: 0.1762\n",
      "[7/7] Loss: 1.0368 Train Acc: 0.6878\n",
      "Epoch: 65, Test IoU: 0.1832\n",
      "[7/7] Loss: 0.9648 Train Acc: 0.7012\n",
      "Epoch: 66, Test IoU: 0.1823\n",
      "[7/7] Loss: 1.0062 Train Acc: 0.6987\n",
      "Epoch: 67, Test IoU: 0.1948\n",
      "[7/7] Loss: 1.1803 Train Acc: 0.6882\n",
      "Epoch: 68, Test IoU: 0.1899\n",
      "[7/7] Loss: 0.9988 Train Acc: 0.7035\n",
      "Epoch: 69, Test IoU: 0.1877\n",
      "[7/7] Loss: 1.0224 Train Acc: 0.7050\n",
      "Epoch: 70, Test IoU: 0.1888\n",
      "[7/7] Loss: 0.8988 Train Acc: 0.7128\n",
      "Epoch: 71, Test IoU: 0.1928\n",
      "[7/7] Loss: 0.9441 Train Acc: 0.7096\n",
      "Epoch: 72, Test IoU: 0.1925\n",
      "[7/7] Loss: 0.9244 Train Acc: 0.7139\n",
      "Epoch: 73, Test IoU: 0.1922\n",
      "[7/7] Loss: 0.8808 Train Acc: 0.7169\n",
      "Epoch: 74, Test IoU: 0.1932\n",
      "[7/7] Loss: 1.0119 Train Acc: 0.7144\n",
      "Epoch: 75, Test IoU: 0.1950\n",
      "[7/7] Loss: 0.9256 Train Acc: 0.7188\n",
      "Epoch: 76, Test IoU: 0.1970\n",
      "[7/7] Loss: 0.9567 Train Acc: 0.7232\n",
      "Epoch: 77, Test IoU: 0.1981\n",
      "[7/7] Loss: 0.8761 Train Acc: 0.7242\n",
      "Epoch: 78, Test IoU: 0.1904\n",
      "[7/7] Loss: 1.0511 Train Acc: 0.7196\n",
      "Epoch: 79, Test IoU: 0.1953\n",
      "[7/7] Loss: 0.9332 Train Acc: 0.7119\n",
      "Epoch: 80, Test IoU: 0.1900\n",
      "[7/7] Loss: 0.9237 Train Acc: 0.7213\n",
      "Epoch: 81, Test IoU: 0.1944\n",
      "[7/7] Loss: 0.8957 Train Acc: 0.7194\n",
      "Epoch: 82, Test IoU: 0.1916\n",
      "[7/7] Loss: 1.0493 Train Acc: 0.7144\n",
      "Epoch: 83, Test IoU: 0.1908\n",
      "[7/7] Loss: 1.0066 Train Acc: 0.6916\n",
      "Epoch: 84, Test IoU: 0.1715\n",
      "[7/7] Loss: 1.0280 Train Acc: 0.6900\n",
      "Epoch: 85, Test IoU: 0.1790\n",
      "[7/7] Loss: 1.0274 Train Acc: 0.6892\n",
      "Epoch: 86, Test IoU: 0.1905\n",
      "[7/7] Loss: 0.9509 Train Acc: 0.7094\n",
      "Epoch: 87, Test IoU: 0.1925\n",
      "[7/7] Loss: 0.9645 Train Acc: 0.7090\n",
      "Epoch: 88, Test IoU: 0.1937\n",
      "[7/7] Loss: 0.9645 Train Acc: 0.7037\n",
      "Epoch: 89, Test IoU: 0.1706\n",
      "[7/7] Loss: 0.9407 Train Acc: 0.7021\n",
      "Epoch: 90, Test IoU: 0.1915\n",
      "[7/7] Loss: 0.9471 Train Acc: 0.7162\n",
      "Epoch: 91, Test IoU: 0.1946\n",
      "[7/7] Loss: 0.9989 Train Acc: 0.7186\n",
      "Epoch: 92, Test IoU: 0.1840\n",
      "[7/7] Loss: 1.0530 Train Acc: 0.6942\n",
      "Epoch: 93, Test IoU: 0.1952\n",
      "[7/7] Loss: 0.9756 Train Acc: 0.7268\n",
      "Epoch: 94, Test IoU: 0.1907\n",
      "[7/7] Loss: 0.9369 Train Acc: 0.7074\n",
      "Epoch: 95, Test IoU: 0.1977\n",
      "[7/7] Loss: 0.9018 Train Acc: 0.7246\n",
      "Epoch: 96, Test IoU: 0.1898\n",
      "[7/7] Loss: 0.8949 Train Acc: 0.7125\n",
      "Epoch: 97, Test IoU: 0.1991\n",
      "[7/7] Loss: 0.8878 Train Acc: 0.7230\n",
      "Epoch: 98, Test IoU: 0.1951\n",
      "[7/7] Loss: 0.9190 Train Acc: 0.7255\n",
      "Epoch: 99, Test IoU: 0.1922\n",
      "[7/7] Loss: 0.9037 Train Acc: 0.7213\n",
      "Epoch: 100, Test IoU: 0.1977\n",
      "Epoch: 100, Test IoU: 0.1977, Time: 7.89s\n",
      "Training time: 10.96m\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "import time\n",
    "begin_time = time.perf_counter()\n",
    "for epoch in range(1, 101):\n",
    "    start_time = time.perf_counter()\n",
    "    train()\n",
    "    iou = test(test_loader)\n",
    "    epoch_time = time.perf_counter() - start_time\n",
    "    print(f'Epoch: {epoch:02d}, Test IoU: {iou:.4f}')\n",
    "print(f'Epoch: {epoch:02d}, Test IoU: {iou:.4f}, Time: {epoch_time:.2f}s')\n",
    "total_time = time.perf_counter() - begin_time\n",
    "print(f'Training time: {(total_time)/60:.2f}m')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lora.lora_state_dict(model), \"checkpoints/smartlab_lora_weights_x3_100_20250424.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
