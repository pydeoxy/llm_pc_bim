{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42783120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4082501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_similarity(ref,message):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')  \n",
    "    embeddings = model.encode([ref, message])\n",
    "    similarity_score = cosine_similarity(\n",
    "        [embeddings[0]], \n",
    "        [embeddings[1]] \n",
    "    )[0][0]\n",
    "\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50b1475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "669c5182",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_file = \"../docs/qa_benchmark.json\"\n",
    "benchmark = read_json(bench_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7200e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_file = \"../docs/qa_records_gemma-1.1-2b.json\"\n",
    "qa = read_json(qa_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7701572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_qa(qa1,qa2):\n",
    "    q_score = text_similarity(qa1[\"Query\"],qa2[\"Query\"])\n",
    "    r_score = text_similarity(qa1[\"Response\"],qa2[\"Response\"])\n",
    "    result = {\"Query\":qa1[\"Query\"],\n",
    "              \"q_score\":q_score,\n",
    "              \"r_score\":r_score}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cf053a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_qa(qa,benchmark):\n",
    "    #q_score_total = 0\n",
    "    r_score_total = 0\n",
    "\n",
    "    for i in range(len(qa)-1):\n",
    "        if qa[i+1][\"Query\"]==benchmark[i+1][\"Query\"]:\n",
    "            qa[i+1][\"r_score\"] = text_similarity(qa[i+1][\"Response\"],benchmark[i+1][\"Response\"])\n",
    "            r_score_total += qa[i+1][\"r_score\"]   \n",
    "    r_score_average = r_score_total/(len(qa)-1)\n",
    "    qa[0][\"r_score_average\"] = r_score_average\n",
    "\n",
    "    return qa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0348c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_files = [\n",
    "    \"qa_records_Llama-3-8B.json\",\n",
    "    \"qa_records_Llama-3.2-3B.json\",\n",
    "    \"qa_records_Llama-3.2-3B_p.json\",\n",
    "    \"qa_records_Llama-3.2-1B.json\",\n",
    "    \"qa_records_Llama-3.2-1B_p.json\",\n",
    "    \"qa_records_Llama-3.1-8B.json\",\n",
    "    \"qa_records_gemma-1.1-2b.json\",\n",
    "    \"qa_records_Qwen2.5-1.5B.json\",\n",
    "    \"qa_records_Qwen2.5-0.5B.json\",\n",
    "    \"qa_records_Phi-4-mini.json\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e415c920",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_files_llama_3_2 = [    \n",
    "    \"qa_records_Llama-3.2-3B_p.json\",\n",
    "    \"qa_records_Llama-3.2-3B_1.json\",\n",
    "    \"qa_records_Llama-3.2-3B_2.json\",\n",
    "    \"qa_records_Llama-3.2-3B_3.json\",\n",
    "    \"qa_records_Llama-3.2-3B_4.json\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31887171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Context Length(k)': '128',\n",
      "  'Model': 'Llama-3.2-3B-Instruct(control tokens)',\n",
      "  'Response Time(s)': '43.70',\n",
      "  'Size(GB)': '6.43',\n",
      "  'r_score_average': 0.6064372857411703},\n",
      " {'Context Length(k)': '128',\n",
      "  'Model': 'Llama-3.2-3B-Instruct',\n",
      "  'Response Time(s)': '40.31',\n",
      "  'Size(GB)': '6.43',\n",
      "  'r_score_average': 0.4558170661330223},\n",
      " {'Context Length(k)': '128',\n",
      "  'Model': 'Llama-3.2-3B-Instruct',\n",
      "  'Response Time(s)': '45.36',\n",
      "  'Size(GB)': '6.43',\n",
      "  'r_score_average': 0.41377802938222885},\n",
      " {'Context Length(k)': '128',\n",
      "  'Model': 'Llama-3.2-3B-Instruct',\n",
      "  'Response Time(s)': '44.38',\n",
      "  'Size(GB)': '6.43',\n",
      "  'r_score_average': 0.4472521295150121},\n",
      " {'Context Length(k)': '128',\n",
      "  'Model': 'Llama-3.2-3B-Instruct',\n",
      "  'Response Time(s)': '51.09',\n",
      "  'Size(GB)': '6.43',\n",
      "  'r_score_average': 0.42030222713947296}]\n"
     ]
    }
   ],
   "source": [
    "qa_records = []\n",
    "for f in qa_files:\n",
    "    qf = read_json(\"../docs/\"+f)\n",
    "    #print(f\"{f} has {len(qf)} items.\")\n",
    "    qf_ev = evaluate_qa(qf,benchmark)\n",
    "    qa_records.append(qf_ev[0])\n",
    "\n",
    "pprint(qa_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7513ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_records_json = \"../docs/qa_records_comparison.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f314f000",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(qa_records_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(qa_records, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0064546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Context Length(k)': '128',\n",
      "  'Model': 'Llama-3.2-3B-Instruct(control tokens)',\n",
      "  'Response Time(s)': '43.70',\n",
      "  'Size(GB)': '6.43',\n",
      "  'r_score_average': 0.6064372857411703},\n",
      " {'Context Length(k)': '128',\n",
      "  'Model': 'Llama-3.2-3B-Instruct',\n",
      "  'Response Time(s)': '40.31',\n",
      "  'Size(GB)': '6.43',\n",
      "  'r_score_average': 0.4558170661330223},\n",
      " {'Context Length(k)': '128',\n",
      "  'Model': 'Llama-3.2-3B-Instruct',\n",
      "  'Response Time(s)': '45.36',\n",
      "  'Size(GB)': '6.43',\n",
      "  'r_score_average': 0.41377802938222885},\n",
      " {'Context Length(k)': '128',\n",
      "  'Model': 'Llama-3.2-3B-Instruct',\n",
      "  'Response Time(s)': '44.38',\n",
      "  'Size(GB)': '6.43',\n",
      "  'r_score_average': 0.4472521295150121},\n",
      " {'Context Length(k)': '128',\n",
      "  'Model': 'Llama-3.2-3B-Instruct',\n",
      "  'Response Time(s)': '51.09',\n",
      "  'Size(GB)': '6.43',\n",
      "  'r_score_average': 0.42030222713947296}]\n"
     ]
    }
   ],
   "source": [
    "data = read_json(qa_records_json)\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f55edb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m min_val \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mmin()\n\u001b[0;32m     29\u001b[0m max_val \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmax_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmin_val\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Avoid division by zero if all values are the same\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     df_scaled[metric] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "# Filter the data to exclude models with '8B' in their names\n",
    "filtered_data = [d for d in data if '8B' not in d['Model'] and 'gemma' not in d['Model']]\n",
    "\n",
    "# Convert the filtered list of dictionaries into a pandas DataFrame\n",
    "df = pd.DataFrame(filtered_data)\n",
    "\n",
    "# Rename the columns for the metrics that will be plotted\n",
    "df.rename(columns={'Response Time(s)': 'Inference Speed', \n",
    "                   'Size(GB)': 'Size Index',\n",
    "                   'r_score_average': 'Average Accuracy'}, inplace=True)\n",
    "\n",
    "# 'Response Time(s)' and 'Size(GB)' have values where smaller is better.\n",
    "# To make the comparison consistent with 'Context Length(k)' and 'r_score_average' (where bigger is better),\n",
    "# we normalize the data. A simple way is to take the inverse,\n",
    "# but to keep it on a similar scale, we'll invert it after scaling.\n",
    "# A more robust solution is to use a Min-Max scaling, which we will do here.\n",
    "\n",
    "# Define the metrics to be scaled and those to keep as-is\n",
    "metrics_to_normalize = ['Inference Speed', 'Size Index']\n",
    "metrics_to_keep = ['Average Accuracy']\n",
    "all_metrics = metrics_to_normalize + metrics_to_keep\n",
    "\n",
    "# Create a copy of the dataframe to store the scaled values\n",
    "df_scaled = df.copy()\n",
    "\n",
    "for metric in metrics_to_normalize:\n",
    "    values = df_scaled[metric].values\n",
    "    min_val = values.min()\n",
    "    max_val = values.max()\n",
    "\n",
    "    if max_val - min_val == 0:\n",
    "        # Avoid division by zero if all values are the same\n",
    "        df_scaled[metric] = 0.5\n",
    "    else:\n",
    "        df_scaled[metric] = (values - min_val) / (max_val - min_val)\n",
    "        # Invert the scale for metrics where lower is better\n",
    "        if metric in ['Inference Speed', 'Size Index']:\n",
    "            df_scaled[metric] = 1 - df_scaled[metric]\n",
    "\n",
    "# Melt the DataFrame to a \"long\" format, which is required for seaborn lineplot\n",
    "df_melted = df_scaled.melt(id_vars='Model', value_vars=all_metrics, var_name='Metric', value_name='Value')\n",
    "\n",
    "\n",
    "# Create the line chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.lineplot(data=df_melted, x='Metric', y='Value', hue='Model', marker='o', style='Model')\n",
    "\n",
    "# Add plot title and labels\n",
    "plt.title('Normalized Performance Comparison of Language Models', fontsize=16)\n",
    "plt.xlabel('Metrics', fontsize=12)\n",
    "plt.ylabel('Normalized Value', fontsize=12)\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
