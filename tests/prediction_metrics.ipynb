{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from scipy.spatial import cKDTree\n",
    "import open3d as o3d\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from pc_seg.pc_label_map import color_map_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert the colormap to get RGB â†’ label index\n",
    "rgb_to_label = {tuple(v[0]): k for k, v in color_map_dict.items()}\n",
    "label_to_name = {k: v[1] for k, v in color_map_dict.items()}\n",
    "\n",
    "def get_label_from_color(color_array):\n",
    "    \"\"\"Match color to closest known semantic color label.\"\"\"\n",
    "    labels = []\n",
    "    for c in color_array:\n",
    "        # Round to 3 decimals to avoid float precision mismatch\n",
    "        key = tuple(np.round(c, 3))\n",
    "        label = rgb_to_label.get(key, -1)  # -1 if unmatched\n",
    "        labels.append(label)\n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GT and predicted point clouds\n",
    "gt_pcd = o3d.io.read_point_cloud(\"../docs/SmartLab_2024_E57_Single_5mm_SEG_colors.ply\")   # full resolution\n",
    "pred_pcd = o3d.io.read_point_cloud(\"../docs/Smartlab_s3dis_label_pointnet2_x6_0.03_20250422.ply\")   # downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([gt_pcd], point_show_normal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([pred_pcd], point_show_normal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_label_counts(label_array, name=\"Point Cloud\"):\n",
    "    print(f\"\\nðŸ“¦ Label counts for {name}:\")\n",
    "    unique_labels, counts = np.unique(label_array, return_counts=True)\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        class_name = label_to_name.get(label, f\"Class {label}\")\n",
    "        print(f\"  {label:2d} ({class_name:10s}): {count} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the point cloud to its min(x,y,z) corner\n",
    " \n",
    "def move_to_corner(points):    \n",
    "    # Find the minimum x, y, z\n",
    "    min_xyz = points.min(axis=0)\n",
    "    # Translate the point cloud so that the min corner becomes the origin\n",
    "    moved_points = points - min_xyz\n",
    "    \n",
    "    return moved_points\n",
    "\n",
    "moved_points = move_to_corner(np.array(gt_pcd.points))\n",
    "gt_pcd.points = o3d.utility.Vector3dVector(moved_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract point coordinates and colors\n",
    "gt_points = np.asarray(gt_pcd.points)\n",
    "gt_origin_colors = np.asarray(gt_pcd.colors)\n",
    "gt_colors = np.round(gt_origin_colors, 1)\n",
    "\n",
    "pred_points = np.asarray(pred_pcd.points)\n",
    "pred_origin_colors = np.asarray(pred_pcd.colors)\n",
    "pred_colors = np.round(pred_origin_colors, 1)\n",
    "\n",
    "# Convert color to semantic labels\n",
    "gt_labels = get_label_from_color(gt_colors)\n",
    "pred_labels = get_label_from_color(pred_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¦ Label counts for Ground Truth:\n",
      "   0 (ceiling   ): 5701035 points\n",
      "   1 (floor     ): 5167644 points\n",
      "   2 (wall      ): 11348550 points\n",
      "   4 (column    ): 173882 points\n",
      "   5 (window    ): 549543 points\n",
      "   6 (door      ): 1451390 points\n",
      "   7 (table     ): 243330 points\n",
      "   8 (chair     ): 99590 points\n",
      "   9 (sofa      ): 190263 points\n",
      "  11 (board     ): 488769 points\n",
      "  12 (clutter   ): 10552776 points\n",
      "\n",
      "ðŸ“¦ Label counts for Prediction:\n",
      "   0 (ceiling   ): 130218 points\n",
      "   1 (floor     ): 120958 points\n",
      "   2 (wall      ): 277981 points\n",
      "   4 (column    ): 5052 points\n",
      "   5 (window    ): 208 points\n",
      "   6 (door      ): 20088 points\n",
      "   7 (table     ): 114 points\n",
      "   8 (chair     ): 7 points\n",
      "  10 (bookcase  ): 63449 points\n",
      "  11 (board     ): 14 points\n",
      "  12 (clutter   ): 248811 points\n"
     ]
    }
   ],
   "source": [
    "# Ground Truth\n",
    "print_label_counts(gt_labels, name=\"Ground Truth\")\n",
    "\n",
    "# Prediction\n",
    "print_label_counts(pred_labels, name=\"Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Overall Accuracy: 0.645\n",
      "\n",
      "ðŸ“‹ Per-Class Accuracy:\n",
      "   0 (ceiling   ): 0.839 (4783406/5701035)\n",
      "   1 (floor     ): 0.947 (4895896/5167644)\n",
      "   2 (wall      ): 0.642 (7283663/11348550)\n",
      "   4 (column    ): 0.000 (0/173882)\n",
      "   5 (window    ): 0.002 (1202/549543)\n",
      "   6 (door      ): 0.110 (158963/1451390)\n",
      "   7 (table     ): 0.009 (2088/243330)\n",
      "   8 (chair     ): 0.000 (0/99590)\n",
      "   9 (sofa      ): 0.000 (0/190263)\n",
      "  11 (board     ): 0.000 (0/488769)\n",
      "  12 (clutter   ): 0.575 (6064681/10552776)\n"
     ]
    }
   ],
   "source": [
    "# Filter out unmatched labels (e.g. -1)\n",
    "valid_mask = (gt_labels != -1)\n",
    "gt_points = gt_points[valid_mask]\n",
    "gt_labels = gt_labels[valid_mask]\n",
    "\n",
    "# KDTree nearest neighbor matching\n",
    "tree = cKDTree(pred_points)\n",
    "_, indices = tree.query(gt_points, k=1)\n",
    "matched_pred_labels = pred_labels[indices]\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy = accuracy_score(gt_labels, matched_pred_labels)\n",
    "print(f\"\\nâœ… Overall Accuracy: {overall_accuracy:.3f}\")\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nðŸ“‹ Per-Class Accuracy:\")\n",
    "class_counts = defaultdict(int)\n",
    "correct_counts = defaultdict(int)\n",
    "\n",
    "for true, pred in zip(gt_labels, matched_pred_labels):\n",
    "    class_counts[true] += 1\n",
    "    if true == pred:\n",
    "        correct_counts[true] += 1\n",
    "\n",
    "for label in sorted(class_counts.keys()):\n",
    "    correct = correct_counts[label]\n",
    "    total = class_counts[label]\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    class_name = label_to_name.get(label, f\"Class {label}\")\n",
    "    print(f\"  {label:2d} ({class_name:10s}): {acc:.3f} ({correct}/{total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     ceiling       0.95      0.84      0.89   5701035\n",
      "       floor       0.97      0.95      0.96   5167644\n",
      "        wall       0.62      0.64      0.63  11348550\n",
      "      column       0.00      0.00      0.00    173882\n",
      "      window       0.28      0.00      0.00    549543\n",
      "        door       0.14      0.11      0.12   1451390\n",
      "       table       0.38      0.01      0.02    243330\n",
      "       chair       0.00      0.00      0.00     99590\n",
      "        sofa       0.00      0.00      0.00    190263\n",
      "    bookcase       0.00      0.00      0.00         0\n",
      "       board       0.00      0.00      0.00    488769\n",
      "     clutter       0.58      0.57      0.58  10552776\n",
      "\n",
      "    accuracy                           0.64  35966772\n",
      "   macro avg       0.33      0.26      0.27  35966772\n",
      "weighted avg       0.67      0.64      0.65  35966772\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Optional: classification report\n",
    "# Ensure all labels used in GT or prediction are covered\n",
    "all_labels = sorted(set(gt_labels) | set(matched_pred_labels))\n",
    "\n",
    "print(classification_report(\n",
    "    gt_labels,\n",
    "    matched_pred_labels,\n",
    "    labels=all_labels,\n",
    "    target_names=[label_to_name.get(i, f\"Class {i}\") for i in all_labels]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.19205701 1.61344356 0.22196501]\n",
      "[5.20587027 5.64833263 1.76726065]\n"
     ]
    }
   ],
   "source": [
    "# Compute the centroid\n",
    "gt_centroid = gt_points.mean(axis=0)\n",
    "pred_centroid = pred_points.mean(axis=0)\n",
    "\n",
    "print(gt_centroid)\n",
    "print(pred_centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.03750002 -4.06758022 -1.56200004]\n",
      "[0.         0.00400019 0.00703452]\n"
     ]
    }
   ],
   "source": [
    "gt_min_xyz = gt_points.min(axis=0)\n",
    "pred_min_xyz = pred_points.min(axis=0)\n",
    "\n",
    "print(gt_min_xyz)\n",
    "print(pred_min_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.25423009826106197\n",
      "Confusion Matrix:\n",
      " [[      0       0       0       0       0       0       0       0]\n",
      " [1833676       0   10518 1992383      34 1645166       0  219258]\n",
      " [  91965       0 4283931  711422       0    2188       0   78138]\n",
      " [1227032       0 4877474 3870461       0  742960       0  630623]\n",
      " [      0       0       0       0       0       0       0       0]\n",
      " [  18970       0   59926     541       0   94445       0       0]\n",
      " [  31691       0  166651  182922       0  163731       0    4548]\n",
      " [1246009       0 3884928 3739601       0 1416037       0  266201]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         0\n",
      "           0       0.00      0.00      0.00   5701035\n",
      "           1       0.32      0.83      0.46   5167644\n",
      "           2       0.37      0.34      0.35  11348550\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.02      0.54      0.04    173882\n",
      "           5       0.00      0.00      0.00    549543\n",
      "          12       0.22      0.03      0.05  10552776\n",
      "\n",
      "    accuracy                           0.25  33493430\n",
      "   macro avg       0.12      0.22      0.11  33493430\n",
      "weighted avg       0.24      0.25      0.21  33493430\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Extract numpy arrays\n",
    "gt_points = np.asarray(gt_pcd.points)\n",
    "gt_colors = np.asarray(gt_pcd.colors)  # RGB in [0.0, 1.0]\n",
    "\n",
    "pred_points = np.asarray(pred_pcd.points)\n",
    "pred_colors = np.asarray(pred_pcd.colors)\n",
    "\n",
    "# Map RGB colors to semantic class labels\n",
    "gt_labels = get_label_from_color(gt_colors)\n",
    "pred_labels = get_label_from_color(pred_colors)\n",
    "\n",
    "# Filter out unmatched labels (optional)\n",
    "valid_mask = (gt_labels != -1)\n",
    "gt_points = gt_points[valid_mask]\n",
    "gt_labels = gt_labels[valid_mask]\n",
    "\n",
    "# KDTree to find nearest predicted point for each GT point\n",
    "tree = cKDTree(pred_points)\n",
    "_, indices = tree.query(gt_points, k=1)\n",
    "\n",
    "# Predicted labels by nearest point\n",
    "matched_pred_labels = pred_labels[indices]\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(gt_labels, matched_pred_labels))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(gt_labels, matched_pred_labels))\n",
    "print(\"Classification Report:\\n\", classification_report(gt_labels, matched_pred_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
